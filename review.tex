\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Task Offloading of Edge Computing Based on Reinforcement Learning: A Survey}

\author{Pu Yangyizhen, Xie Yijie, Zheng Tongzhou, Zhou Xufei}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}
% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Cloud Computing, January~2025}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
abstract here
\end{abstract}

\begin{IEEEkeywords}
Edge Computing, Task Offloading, Reinforcement Learning, Multi-Agent Reinforcement Learning.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{E}{dge} computing refers to a distributed and open platform that integrates the core capabilities of networking, 
computing, storage, and applications on the network edge side close to the data source. 
It provides edge intelligence services nearby to meet the key requirements of industry digitalization in aspects such as agile connection, 
real-time services, data optimization, application intelligence, security, and privacy protection~\cite{zheng01}. 
Traditional cloud computing generally centralizes data in data centers for processing, 
while edge computing assigns some computing tasks to the network edge for processing, handling data where it is generated.

The importance of edge computing is becoming increasingly prominent. 
Taking the booming autonomous driving in recent years as an example, autonomous driving requires extremely high real-time performance. 
If traditional cloud computing is used, the delay caused by vehicles transmitting data to the data center for processing and then 
getting the results back is unacceptable and may even lead to traffic accidents. 
However, edge computing enables vehicles to process data locally and make timely decisions, ensuring real-time performance.

Meanwhile, edge computing has also become important in terms of data privacy protection. 
For instance, many sensitive data (such as medical and health data, financial transaction data, etc.) are not suitable 
for being transmitted to remote cloud for processing. 
Edge computing allows data processing and analysis to be carried out on local devices or edge servers, reducing the risk of data leakage.

Although edge computing is gradually becoming a key technology to support various emerging applications and business models, 
with the development of mobile devices and the Internet of Things, 
the amount of data that devices need to handle and computing tasks are becoming increasingly complex. 
To further improve the processing efficiency, task offloading has emerged.

Task offloading means transferring the computing tasks of a device to other devices or 
servers with stronger computing capabilities for processing. 
It is common to offload the tasks of mobile devices to edge servers or the cloud~\cite{zheng03}.

However, in actual dynamic environments, task offloading faces numerous challenges:

\begin{itemize}
    \item \textbf{Latency Challenge}: In dynamic environments, the network state can change at any time. Situations such as network congestion and unstable signals may lead to a significant increase in data transmission latency. For example, in the Internet of Vehicles scenario, vehicles are in a high-speed moving state and network connections are constantly switching. During the process of offloading tasks from vehicles to edge servers or the cloud, it is difficult to predict and control the data transmission latency \cite{zheng02}.

    \item \textbf{Energy Consumption Challenge}: Although offloading tasks to external computing resources can utilize stronger computing capabilities, the energy consumption issue during the task offloading process also needs to be considered. Packaging and transmitting data as well as interacting with external resources all consume energy. Especially for mobile devices with limited battery power, an unreasonable task offloading strategy may cause a sharp increase in device energy consumption and shorten the device's battery life \cite{zheng04}.

    \item \textbf{Device Heterogeneity Challenge}: The development of mobile devices and the Internet of Things has led to an increasingly diverse range of devices, with huge differences in hardware architectures, computing capabilities, storage capacities, and so on. This heterogeneity has caused two main problems. 
    \begin{itemize}
        \item On the one hand, the software and hardware environments of different devices are different. If the offloaded tasks are to run smoothly on the target devices, complex compatibility adaptation work is required.
        \item On the other hand, the performance and computing capabilities of different devices also vary. If the same tasks are assigned to each device, it will lead to a decline in performance.
    \end{itemize}
    Therefore, tasks need to be reasonably allocated according to the actual situation of the devices.
\end{itemize}

Faced with a series of complex challenges such as latency, energy consumption, and device heterogeneity that task offloading encounters in dynamic environments, 
traditional methods based on rules or static optimization are gradually proving to be inadequate. These traditional methods often struggle to cope with the dynamic changes of the environment and complex system constraints, and thus cannot achieve optimal performance.

Against this background, reinforcement learning, as a powerful machine learning technique, has provided highly promising ideas and methods for solving the problem of task offloading.

The core of reinforcement learning lies in the fact that an agent interacts with the environment, 
continuously tries different actions, and learns the optimal strategy based on the obtained reward feedback, 
so as to maximize the cumulative reward in the long term~\cite{zheng05}. 
This learning mode is inherently suitable for the dynamically changing task offloading scenarios 
because it can dynamically adjust task offloading decisions according to the real-time environmental states 
(such as network conditions, device loads, etc.), achieving self-adaptation to complex environments.

For example, when dealing with the latency issue, the reinforcement learning algorithm can, through continuous trial-and-error learning, choose to offload tasks to local devices, edge servers, or the cloud according to the real-time latency situation of the current network, so as to ensure that the overall task execution latency is minimized~\cite{zheng06}.

Through continuous learning and optimization, reinforcement learning can help agents find near-optimal task offloading strategies, providing a powerful technical support for solving the difficulties faced by task offloading in dynamic environments. It is expected to become a key tool for promoting the development of edge computing task offloading technology.

\section{Research Status of Reinforcement Learning in Task Offloading}

\subsection{The Foundation of Reinforcement Learning}
The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations.While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. To address this issue, researchers proposed the Deep Q-Network (DQN), which combines deep learning with Q-learning to enable effective decision-making in complex environments~\cite{pyyz001}.

DQN uses one particularly successful architecture, the deep convolutional network, which uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive fields—inspired by Hubel and Wiesel’s seminal work on feedforward processing in early visual cortex—thereby exploiting the local spatial correlations present in images, and building in robustness to natural transformations such as changes of viewpoint or scale.

Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator, such as a neural network, is used to represent the action-value (Q) function. This instability has several causes, including the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values. There is a novel variant of Q-learning that employs two key ideas:

\begin{itemize}
\item \textbf{Experience Replay}: This biologically inspired mechanism randomizes the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Furthermore, this mechanism stores the agent's experiences (state, action, reward, next state) in a replay buffer. During training, small batches of samples are randomly drawn from this buffer to update the network, breaking the correlations between consecutive data points and improving learning efficiency.
\item \textbf{Iterative Update}: This method adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target. 
\end{itemize}

To stabilize the training process, DQN employs the concept of a target network. The weights of the target network are updated periodically (e.g., every few steps) to reduce fluctuations in the Q-values. By using a target network, DQN maintains a relatively stable target during updates, which enhances learning stability and convergence speed.The training process of DQN includes the following steps:

\subsubsection{Action Selection}The agent selects an action based on the current state, typically using an $\epsilon$-greedy strategy, where it randomly selects an action with a certain probability (exploration) and selects the action with the highest Q-value with a certain probability (exploitation).

\subsubsection{Action Execution}The agent executes the selected action in the environment and observes the next state and reward.

\subsubsection{Experience Storage}The current experience state,action,reward,nextstate is stored in the replay buffer.

\subsubsection{Q-value Update}A small batch of samples is randomly drawn from the replay buffer, and the Q-values are updated using the Bellman equation. Specifically, DQN updates the network weights by minimizing the mean squared error of the Q-values.

The success of DQN demonstrates the potential of combining deep learning with reinforcement learning, laying the foundation for subsequent research. This method not only achieved significant results in gaming but also inspired applications in other fields, such as robotics and autonomous driving.

Dueling Double Deep Q-Network (D3QN) is an improvement over the Deep Q-Network (DQN) designed to enhance the learning efficiency and stability of reinforcement learning in complex environments. D3QN combines the advantages of Double Q-learning and the dueling network architecture, primarily consisting of the following key points:

\begin{itemize}
\item \textbf{Double Q-Learning}:D3QN introduces the concept of Double Q-learning to address the overestimation problem of Q-values in traditional Q-learning. By using two independent Q-networks (the main network and the target network), D3QN selects actions using one network and evaluates the value of those actions using the other network, thereby reducing bias in the Q-values.
\item \textbf{Dueling Network Architecture}:D3QN employs a dueling network architecture that decomposes the Q-value function into two components: the state value function (V) and the advantage function (A)~\cite{pyyz002}. This decomposition allows the network to better capture the relative value of different actions. Specifically, the Q-value can be expressed as: \begin{equation}
    \mathit{Q}(s, a) = \mathit{V}(s) + \mathit{A}(s, a)
\end{equation}

By using this approach, D3QN provides a more stable learning signal, especially in situations where states are similar but actions differ.
\item \textbf{Experience Replay and Target Network}:D3QN continues to utilize the experience replay mechanism and target network from DQN to further enhance learning stability~\cite{pyyz003}.Experience replay breaks the correlations between consecutive data points by randomly sampling from a replay buffer, while the target network is updated periodically to reduce fluctuations during training.
\item \textbf{Performance Improvement}:Experiments have shown that D3QN performs exceptionally well in various reinforcement learning tasks, particularly in environments that require handling complex state and action spaces. By combining Double Q-learning and the dueling network architecture, D3QN converges to optimal policies more quickly and improves overall performance.
\end{itemize}

\subsection{Applications of Classical Reinforcement Learning}
Q-learning The breakthrough in Machine Learning (ML) techniques and the popularity of the Internet of Things (IoT) has increased interest in applying Artificial Intelligence (AI) techniques to the new paradigm of Edge Computing. One of the challenges in edge computing architectures is the optimal distribution of the generated tasks between the devices in each layer (i.e., cloud-fog-edge)~\cite{pyyz004}.

To make efficient use of devices in edge computing architecture, it is necessary to use a computational offloading framework that optimizes the allocation of tasks to computing devices in the best possible way. The procedure for deciding the best allocation of tasks to devices is called “task assignment problem”and is a combinatorial optimization problem defined as the process of determining where the computation of each task is performed in order to minimize certain parameters, such as the aforementioned latency and energy consumption, an important topic for an eco-friendly future~\cite{pyyz006} and a key component of the green computing philosophy.

The task assignment problem (TAP) can be formulated as a Generalized Assignment Problem (GAP)~\cite{pyyz007}, which is an NP-hard. Among the traditional methods for solving GAP, new methods based on dynamic programming and machine learning techniques have emerged, such as reinforcement learning (RL) and neural network reinforcement learning (Deep RL). Reinforcement learning optimizes task offloading through the following key aspects:

\begin{itemize}
\item \textbf{Definition of State and Action}:Researchers define a state space that includes various factors such as the device's computing capability, network latency, task size, and complexity. This state information helps the agent understand the characteristics of the current environment, enabling it to make more informed decisions. The action space includes different offloading strategies, such as choosing to process tasks locally, offloading tasks to edge servers, or selecting different edge servers for processing. By defining multiple possible actions, the agent can explore various offloading strategies.
\item \textbf{Reward Mechanism}:Researchers design a reward mechanism aimed at encouraging the agent to select offloading strategies that minimize latency and energy consumption. After executing each action, the agent receives corresponding rewards or penalties based on task completion time, energy consumption, and other performance metrics. This feedback mechanism prompts the agent to continuously adjust its strategy to optimize overall performance.
\item \textbf{Multi-Layer Guided Mechanism}:The multi-layer guided mechanism proposed allows the agent to learn at different decision-making levels. Each layer's guiding strategy targets specific task features and environmental states, helping the agent learn and make decisions more effectively in complex task offloading scenarios. This hierarchical structure enables the agent to converge to the optimal strategy more quickly.
\item \textbf{Reinforcement Learning Algorithm}:Specific reinforcement learning algorithms (such as Q-learning or Deep Q-Networks) can train the agent. Through interaction with the environment, the agent continuously updates its strategy to adapt to the dynamically changing edge computing environment.
\end{itemize}

All innovation comes with challenges, and IoT and mobile technologies are no exception. One of them is the latency problem, some mobile applications, such as autonomous driving or virtual reality, need to meet a maximum delay requirement, so in some cases it is not possible to send the data to the cloud. Reinforcement learning has the following advantages in addressing these issues:

\subsubsection{Adaptability}
Reinforcement learning can dynamically adjust its strategy based on changes in the environment. In edge computing, network conditions, device loads, and user demands may change frequently, and reinforcement learning can learn and adapt in real-time to optimize task offloading and resource allocation.

\subsubsection{Efficient Resource Management}
Edge computing often faces challenges with limited resources. Reinforcement learning can learn optimal resource allocation strategies to maximize resource utilization, reduce latency, and minimize energy consumption. For example, agents can learn how to select the most suitable computing node to process tasks under different network conditions.

\subsubsection{Online Learning Capability}
Reinforcement learning can learn during operation, meaning it can continuously update and optimize its strategy in edge computing environments without needing offline training. This online learning capability allows the system to quickly adapt to new tasks and changes in the environment.

\subsubsection{Reduced Communication Overhead}
In edge computing, transmitting data to the cloud for processing can lead to high latency and bandwidth consumption. By using reinforcement learning on edge devices, agents can decide when and how to offload tasks, thereby reducing unnecessary communication overhead.

\subsubsection{Support for Multi-task Processing}
There may be multiple tasks occurring simultaneously in edge computing environments. Reinforcement learning can learn how to effectively schedule and allocate resources among multiple tasks to achieve higher system throughput and efficiency.

Through these methods, reinforcement learning achieves an adaptive and intelligent decision-making process in task offloading, allowing edge computing systems to handle tasks more efficiently. In the future, people may explore alternatives of knowledge transfer and federated learning to develop distributed decision systems in 5G networks and Computing Continuum paradigm~\cite{pyyz008}. Another promising future research direction is to apply neural networks to improve the performance of Q-Learning, thus overcoming some limitations such as a low number of states and discrete variables.

\subsection{Applications of Deep Reinforcement Learning}

In the research on task offloading, enhanced reinforcement learning methods have also been continuously developed and innovated. Here, we mainly introduce two methods. One is the Double Dueling DQN (D3QN), which demonstrates unique advantages in solving offloading problems in highly dynamic environments. The other is Multi-Agent Reinforcement Learning (MARL), which plays an important role in edge collaboration scenarios.

Edge computing networks face difficulties such as limited device resources and dynamic environmental changes. 
Zhang et al.~\cite{zheng07} modeled the online offloading problem as a Markov Decision Process (MDP) and, 
on this basis, proposed the C - D3QN algorithm~\cite{zheng07}. 
This algorithm combines the D3QN algorithm with a clustering algorithm and effectively improves the 
decision-making performance in highly dynamic environments by introducing a double dueling mechanism.

In terms of algorithm design, the centralized agent collects information on edge servers, mobile terminal devices, and channel states to form the state space. The action space covers the task execution decisions of terminal devices, and the reward function combines incentives such as task completion latency, terminal energy consumption, and task success rate to guide the agent's decisions.

When it comes to exploration and action selection, the agent adopts the $\varepsilon$-greed method. In the early 
stage, it randomly explores with a high probability, and in the later stage, it selects the action 
with the maximum Q value. The C - D3QN model introduces a fixed target mechanism, initializing the 
online learning and target networks. The online learning network updates parameters in real time, and 
the target network is periodically updated to the parameter values of the online learning network to stabilize training and avoid overestimation of Q values. Meanwhile, the dueling mechanism changes the network output from a single Q value to a value function V value and an advantage function A value, enabling a more accurate estimation of Q values and enhancing the adaptability to highly dynamic environments.

Experiments show that the C - D3QN algorithm has obvious advantages over DQN in complex dynamic scenarios. It has a faster convergence speed and can quickly adapt to environmental changes for decision-making. In terms of task completion latency, it selects appropriate execution methods according to tasks and resources to reduce latency. Regarding terminal energy consumption, it makes reasonable offloading decisions to avoid excessive energy consumption and extend the battery life.

As the scale of edge computing expands, the collaboration among edge nodes becomes increasingly important. Multi-Agent Reinforcement Learning (MARL) can better cope with complex edge collaboration scenarios through the interaction and collaboration among multiple agents.

In edge computing task offloading, different edge servers or devices can be regarded as different agents. They work together to complete the offloading tasks while also taking into account factors such as the communication costs and differences in computing capabilities among different devices and servers~\cite{zheng08}. The MARL approach allows each agent to make decisions based on its own observations and local information, and at the same time communicate and collaborate with other agents to gradually achieve the globally optimal collaboration strategy.

However, MARL also faces challenges. Conflicts of interest among agents may lead to difficulties in collaboration, and an increase in the number of agents causes the state and action spaces to grow exponentially, resulting in the "curse of dimensionality" and increasing the computational complexity and convergence difficulty. To address these challenges, researchers have proposed various solutions. A reasonable reward mechanism can adjust the reward function, rewarding agents that actively collaborate and punishing selfish behaviors, prompting them to pursue their own interests while achieving the goals of the system~\cite{zheng09}. The hierarchical MARL architecture divides agents into different layers, with higher-level agents coordinating lower-level ones, reducing complexity and improving collaboration efficiency~\cite{zheng10}.

In practical applications, MARL algorithms can support edge devices to coordinate the allocation of task shares and resource utilization in real time. They can also assist vehicles and Roadside Units (RSUs) in sharing traffic information and facilitating task offloading. In the future, MARL algorithms are also expected to play a significant role in the Internet of Things.

In conclusion, the applications of Double Dueling DQN and Multi-Agent Reinforcement Learning in the field of edge computing task offloading provide powerful means to solve the problems in highly dynamic environments and edge collaboration. However, they also face numerous challenges at the same time. Future research needs to further optimize the algorithms to adapt to the continuously developing demands of edge computing and promote the application and development of edge computing technology in a broader range of fields.


\section{Optimization and Innovation of Reinforcement Learning Methods}

\subsection{Method optimization}

The task offloading technology plays a crucial role in edge computing systems, directly impacting both system efficiency and user experience. However, in real-world applications, task offloading faces several challenges, particularly in the context of delay-sensitive tasks. In areas such as autonomous driving and healthcare, task deadlines are stringent, and any delays may result in serious consequences. Additionally, the heterogeneity of devices and tasks introduces significant complexity to task offloading. Differences in resource capacities, task complexity, and data characteristics complicate the formulation of a unified offloading strategy. Achieving a balance between generic policies and individualized requirements remains a significant challenge, as centralized policy training, despite its low overhead, lacks specificity, while individualized policies incur high training costs. The cold-start problem further exacerbates these issues; newly introduced devices may not be covered by existing policies, requiring retraining and adding to system overhead.

To address these challenges, several approaches have been proposed. The DOQO method, as introduced by Chen et al., enhances real-time performance and optimizes resource utilization by dynamically adjusting offloading decisions~\cite{zhou01}. The Transfer Reinforcement Learning (TRL) framework, developed by Shuai et al., combines transfer learning and domain adaptation to reduce training costs and improve model adaptability~\cite{zhou02}. The C-D3QN algorithm, proposed by Zhang et al., uses K-means clustering and deep reinforcement learning to optimize task offloading, significantly enhancing exploration efficiency~\cite{zhou03}. Robles-Enciso et al. introduced the Multi-Layer Reinforcement Learning (ML-RL) system, which improves task offloading efficiency through cooperation among the edge, fog, and cloud layers~\cite{zhou04}. Wang et al. proposed MRLCO, a meta-weighted learning-based system, which rapidly adapts to dynamic environments while maintaining high sample efficiency and stable performance.~\cite{zhou05}

\subsubsection{DOQO}

DOQO is a real-time, sense-dependent task offloading method that primarily addresses the task offloading problem in dynamic environments. The method optimizes the task offloading process by modeling mobile applications as directed acyclic graphs (DAGs) to capture inter-task dependencies and parallelism.DOQO employs a deep Q-network (DQN) algorithm to evaluate the Q-value of the offloading decision and dynamically adjust the offloading strategy. A distinguishing feature of DOQO is its approach to task prioritization, which eschews preset priorities in favor of a flexible scheduling mechanism that responds to real-time changes in the environment. This flexibility enables the system to swiftly generate offloading plans and make timely adjustments, a capability that traditional methods often lack. 

However, DOQO presents several challenges. The uncertainty in task scheduling and execution location leads to an expansive solution space, increasing computational complexity. Conflicting priorities may also hinder the efficient execution of parallel tasks. Furthermore, DOQO's disregard for wireless channel fading effects may limit its effectiveness in real-world mobile environments. Despite these limitations, DOQO remains a viable method, and further optimization is required, particularly in its adaptability to complex environments.~\cite{zhou01}

\subsubsection{TRL Framework}

The TRL framework proposes a task offloading method that combines migration learning with domain adaptive mechanisms. The objective of this combination is to reduce training overhead and improve model adaptation. The framework is composed of two modules: a data alignment module and a reinforcement learning module. The data alignment module maps data from disparate devices to the reproducing kernel Hilbert space (RKHS) through the domain adaptation mechanism, thereby reducing data distribution differences and enhancing the sharing capability among devices. The reinforcement learning module employs an Actor-Critic architecture, enabling devices to make globally optimal decisions under limited observation conditions and optimize strategies through feedback. The incorporation of Migration Learning has been demonstrated to expedite the convergence of Deep Reinforcement Learning (DRL) models, thereby facilitating the rapid adaptation of devices to novel environments and enhancing execution efficiency. 

While the TRL framework effectively improves adaptability and convergence, there is room for improvement, particularly in adapting to high-mobility devices and optimizing resource utilization with varying task granularity. Thus, while the TRL framework presents an effective solution for dynamic task offloading, further enhancements are necessary for its practical implementation.~\cite{zhou02}

\subsubsection{C-D3QN Algorithm}

The C-D3QN algorithm combines K-means clustering with deep reinforcement learning (DRL) to optimize task offloading in dynamic environments. It uses a centralized decision-making agent to collect global state information, ensuring globally optimal decisions. This centralized framework enables efficient coordination among devices, addressing the challenge of suboptimal local decisions common in traditional methods.C-D3QN leverages K-means clustering to partition devices, reducing the action space and enhancing exploration efficiency, especially with a larger number of devices or tasks. By compressing the decision space, it facilitates more effective resource allocation, particularly when task requirements or device resources are complex.The reward function in C-D3QN accounts for energy consumption and penalties, dynamically adjusting offloading decisions based on task load and device location, thereby optimizing resource allocation. This flexibility allows the system to adapt to environmental changes, improving overall efficiency.

Despite its benefits, C-D3QN is limited by its reliance on a coarse-grained task model, hindering fine-grained offloading optimization. It also lacks support for continuous action space models, which limits its applicability in some scenarios. Future research should incorporate fine-grained task models and policy gradient algorithms to further optimize resource utilization and performance.~\cite{zhou03}

\subsubsection{ML-RL}

The ML-RL system enhances task offloading efficiency by leveraging collaborative agents across edge, fog, and cloud layers. Each layer is responsible for tasks with varying latency and computational power requirements: the edge layer handles low-latency tasks, the fog layer processes medium-latency tasks, and the cloud layer manages tasks requiring high computational power. This collaboration allows lower-layer agents to delegate offloading decisions to upper-layer agents, ensuring globally optimal decisions when local knowledge is insufficient.

The ML-RL system integrates greedy methods with reinforcement learning to improve offloading efficiency and convergence speed. Greedy methods, while providing suboptimal solutions through heuristic rules, rely heavily on real-time information, which can result in unstable execution. In contrast, reinforcement learning enables edge devices to improve their offloading strategy over time by interacting with the environment, maximizing long-term rewards. However, the system's inability to support task decomposition and parallel processing limits its applicability in complex scenarios. Enhancing task granularity processing and exploring more flexible policy learning algorithms would further improve system performance.~\cite{zhou04}

\subsubsection{MRLCO}

MRLCO is an innovative task offloading solution leveraging Meta-Reinforcement Learning (MRL) and sequence-to-sequence (seq2seq) neural networks. The core innovation of MRLCO lies in its ability to quickly adapt to new environments with minimal data, reducing training time and computational overhead. This makes it significantly more efficient than traditional reinforcement learning methods.

The method models task offloading as multiple Markov Decision Processes (MDPs) and transforms the decision-making into a sequence prediction problem. Seq2seq networks are used to enhance decision flexibility, while an attention mechanism addresses information loss, improving decision-making accuracy and robustness. This flexibility allows MRLCO to adapt to dynamic challenges such as network fluctuations and device disconnections, ensuring accurate offloading decisions.Additionally, MRLCO features an adaptive client selection algorithm that filters underperforming clients, improving system stability and resource utilization. This ensures high offloading performance even in unstable environments.

Despite its advantages, MRLCO's performance still falls short of the optimal solution. Future research should explore more efficient non-strategic MRL techniques to further enhance decision-making and training efficiency.~\cite{zhou05}


To further optimize existing methods, it is essential to develop fine-grained task offloading models. For instance, task decomposition based on Directed Acyclic Graphs (DAGs) could enhance decision-making capabilities when combined with policy gradient learning.~\cite{zhou03} Additionally, research should focus on addressing the impact of high device mobility and dynamic network environments on task offloading performance to improve algorithm adaptability in real-world scenarios~\cite{zhou01, zhou02}. Lastly, multi-objective optimization techniques should be prioritized to balance delay, energy consumption, and resource utilization, ultimately enhancing offloading efficiency and system stability in edge computing environments.~\cite{zhou03}

\subsection{Compression of action space and state space}

\section{Current Challenges and Future Prospects}
See for resources on formatting math into text and additional help in working with \LaTeX .

\subsection{Challenges}

The application of reinforcement learning (RL) for edge computing task offloading encounters several challenges in practical settings. These challenges primarily involve the design of scenarios, algorithmic complexity, mobility issues, device and network heterogeneity, security concerns, and communication interference, among others.

\subsubsection{Scenario Design Challenges}
One of the primary challenges in applying RL to edge computing task offloading lies in scenario design~\cite{zhou06}. While numerous studies rely on simulations and analyses based on static scenarios, real-world scenarios are inherently dynamic. Factors such as device mobility, network bandwidth fluctuations, and variations in user behavior can affect task loads, revealing the limitations of static offloading decisions. In contrast, RL methods, known for their adaptability in dynamic environments, offer a promising solution through the implementation of adaptive strategies. However, the ability to effectively model and adapt to environmental changes, ensuring that offloading decisions can be made in real time without introducing delays or inefficiencies, remains a critical problem that requires further development.

\subsubsection{Algorithm Complexity and Latency Problem}
The complexity of RL algorithms and the associated latency are significant challenges in edge computing task offloading~\cite{zhou07}. In time-varying environments, RL often requires substantial computational resources and time to explore the high-dimensional action space, leading to slow processing speeds and delayed convergence during training. This issue is particularly problematic in real-time edge computing applications, where latency is a critical concern. Overly complex algorithms can increase computation times for offloading decisions, undermining system responsiveness. Therefore, the development of more efficient RL algorithms that reduce computational overhead while maintaining offloading effectiveness is crucial to improving the overall efficiency of task offloading in edge computing systems.

\subsubsection{Mobility Challenges of Terminal Devices}
The mobility of terminal devices presents significant challenges, particularly when these devices move across different service areas, requiring seamless task offloading and system stability~\cite{zhou08, zhou09}. Devices may experience network fluctuations, bandwidth variations, and other issues during movement, which can negatively impact offloading efficiency. RL has the potential to address these challenges by enabling dynamic adjustments to offloading strategies. However, a critical issue remains the ability to maintain both timeliness and accuracy in offloading decisions, especially in environments characterized by device mobility and network fluctuations. Ensuring the adaptability and robustness of RL models is essential to enable effective task offloading in diverse network conditions.

\subsubsection{Heterogeneous Network and Device Compatibility Issues}
Heterogeneous device and network environments represent a significant obstacle for RL-based task offloading in edge computing~\cite{zhou09}. The rapid proliferation of new devices and network topologies, driven by advancements in IoT and 5G technologies, has rendered traditional offloading strategies less effective due to the diversity of devices and networks. RL-based offloading solutions must address these compatibility issues to ensure the effectiveness of the offloading process. Developing adaptable and compatible offloading schemes that maintain efficiency across a wide variety of devices and network environments is a key challenge in current RL research for edge computing.

\subsubsection{Security Challenges}
Security remains a critical concern in edge computing task offloading~\cite{zhou07, zhou08, zhou09}. The distributed nature of edge computing environments introduces vulnerabilities that can be exploited by malicious actors, especially in multi-tenant settings where system failures at a single point can lead to cascading effects. Ensuring the security of data during the offloading process is paramount to prevent data leakage or tampering. The integration of security measures into RL-based offloading decision-making processes is essential to mitigate potential risks and enhance system resilience against adversarial attacks. Addressing these concerns is crucial for the practical deployment of RL in edge computing systems.

\subsubsection{Communication Interference and Resource Management}
The increasing number of devices in edge computing environments has led to a rise in network interference, which exacerbates the challenges of task offloading due to resource contention and communication congestion~\cite{zhou08}. RL offers a promising approach to mitigate these issues by enabling dynamic resource allocation. However, a major challenge remains in optimizing resource distribution across multiple devices and complex network environments to ensure successful task offloading and improve quality of service. Developing RL algorithms that effectively balance offloading, resource allocation, and interference management is essential to maintain system stability and efficiency.

The challenges outlined above are inherently linked to the practical implementation of RL-based task offloading in edge computing. Addressing these issues will significantly enhance the feasibility and efficiency of RL-based task offloading systems. Solving these challenges not only provides theoretical grounding but also offers technological advancements, paving the way for broader adoption of edge computing technologies in practical applications.

\subsection{Future Prospects}

\section{Conclusion}
The conclusion goes here.


% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibliography{ref}

% \end{thebibliography}


\vfill

\end{document}


